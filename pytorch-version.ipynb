{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from torch.autograd import Variable\n",
    "import csv\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(fpath, label_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "    label_name=[]\n",
    "    label_range = []\n",
    "    with open(label_path) as txt_file:\n",
    "        reader = txt_file.readlines()\n",
    "        for index, line in enumerate(reader):\n",
    "            label_txt = line.strip()\n",
    "            if label_txt not in label_name:\n",
    "                label_name.append(label_txt)\n",
    "                label_range.append(index)\n",
    "            label_id = label_name.index(label_txt)\n",
    "            labels.append(label_id)\n",
    "        label_range.append(len(reader))\n",
    "\n",
    "    with codecs.open(fpath, 'r', 'utf-8', errors='ignore') as f:\n",
    "        lines = f.readlines()\n",
    "        for idx,l in enumerate(lines):\n",
    "            l = l.rstrip()\n",
    "            data.append([l.split(' '), labels[idx]])\n",
    "    return data,label_name,label_range\n",
    "data,label_name,label_range = load_data('/home/shiyang/WeSHClass/math/dataset.txt', '/home/shiyang/WeSHClass/math/labels.txt')\n",
    "perm = np.random.permutation(len(data))\n",
    "train_perm = perm[:100]\n",
    "test_perm = perm[100:]\n",
    "train_data = []\n",
    "test_data = []\n",
    "for index in train_perm:\n",
    "    train_data.append(data[index])\n",
    "for index in test_perm:\n",
    "    test_data.append(data[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence maxlen 717\n"
     ]
    }
   ],
   "source": [
    "max_sentence_len = max([len(sentence) for sentence, _ in data[:]])\n",
    "print('sentence maxlen', max_sentence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, word2vec_path, out_chs, filter_heights,num_class):\n",
    "        super(Net, self).__init__()\n",
    "        word_embedding = pd.read_csv(filepath_or_buffer=word2vec_path, header=None, sep=\" \", quoting=csv.QUOTE_NONE)\n",
    "        embedding = word_embedding.values[:,1:]\n",
    "        dict_len, embed_size = embedding.shape\n",
    "        dict_len += 1\n",
    "        unknown_word = np.zeros((1, embed_size))\n",
    "        concat_embedding = torch.from_numpy(np.concatenate([unknown_word, embedding], axis=0).astype(np.float32))\n",
    "        self.embedding = nn.Embedding(num_embeddings=dict_len, embedding_dim=embed_size).from_pretrained(concat_embedding)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.my_dict = ['中文']\n",
    "        self.my_dict.extend(list(word_embedding.values[:,0]))\n",
    "        # nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, ...\n",
    "        self.conv = nn.ModuleList([nn.Conv2d(1, out_chs, (fh, embed_size)) for fh in filter_heights])\n",
    "        self.dropout = nn.Dropout(.5)\n",
    "        self.fc1 = nn.Linear(out_chs*len(filter_heights), num_class)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) # (N, seq_len, embd_dim)\n",
    "        x = x.unsqueeze(1) # (N, Cin, W, embd_dim), insert Channnel-In dim\n",
    "#         import pdb;\n",
    "#         pdb.set_trace()\n",
    "        # Conv2d\n",
    "        #    Input : (N,Cin, Hin, Win )\n",
    "        #    Output: (N,Cout,Hout,Wout) \n",
    "        # squeeze(3) means 2D to 1D; (N,Cout,Hout,Wout) -> [(N,Cout,Hout==seq_len)] * len(filter_heights)\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.conv]\n",
    "        # max_pool1d(input, kernel_size, ..\n",
    "        # (N, Cout, seq_len) --(max_pool1d)--> (N, Cout, 1) --(squeeze(2))--> (N, Cout)\n",
    "        # [(N, Cout)]  len(filter_heights)\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n",
    "        x = torch.cat(x, 1) # (N, Cout*len(filter_heights))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 3.422\n",
      "epoch: 1, loss: 3.285\n",
      "epoch: 2, loss: 2.887\n",
      "epoch: 3, loss: 2.948\n",
      "epoch: 4, loss: 2.779\n",
      "epoch: 5, loss: 2.626\n",
      "epoch: 6, loss: 2.657\n",
      "epoch: 7, loss: 2.602\n",
      "epoch: 8, loss: 2.616\n",
      "epoch: 9, loss: 2.768\n",
      "epoch: 10, loss: 2.576\n",
      "epoch: 11, loss: 2.449\n",
      "epoch: 12, loss: 2.562\n",
      "epoch: 13, loss: 2.534\n",
      "epoch: 14, loss: 2.471\n",
      "epoch: 15, loss: 2.386\n",
      "epoch: 16, loss: 2.230\n",
      "epoch: 17, loss: 2.263\n",
      "epoch: 18, loss: 2.275\n",
      "epoch: 19, loss: 2.227\n",
      "epoch: 20, loss: 2.265\n",
      "epoch: 21, loss: 2.182\n",
      "epoch: 22, loss: 2.279\n",
      "epoch: 23, loss: 2.242\n",
      "epoch: 24, loss: 2.140\n",
      "epoch: 25, loss: 2.055\n",
      "epoch: 26, loss: 2.127\n",
      "epoch: 27, loss: 2.090\n",
      "epoch: 28, loss: 2.057\n",
      "epoch: 29, loss: 2.016\n",
      "epoch: 30, loss: 1.983\n",
      "epoch: 31, loss: 1.902\n",
      "epoch: 32, loss: 1.943\n",
      "epoch: 33, loss: 1.907\n",
      "epoch: 34, loss: 1.868\n",
      "epoch: 35, loss: 1.868\n",
      "epoch: 36, loss: 1.982\n",
      "epoch: 37, loss: 1.988\n",
      "epoch: 38, loss: 1.817\n",
      "epoch: 39, loss: 1.814\n",
      "epoch: 40, loss: 1.834\n",
      "epoch: 41, loss: 1.696\n",
      "epoch: 42, loss: 1.735\n",
      "epoch: 43, loss: 1.756\n",
      "epoch: 44, loss: 1.670\n",
      "epoch: 45, loss: 1.746\n",
      "epoch: 46, loss: 1.624\n",
      "epoch: 47, loss: 1.674\n",
      "epoch: 48, loss: 1.706\n",
      "epoch: 49, loss: 1.628\n",
      "epoch: 50, loss: 1.509\n",
      "epoch: 51, loss: 1.511\n",
      "epoch: 52, loss: 1.552\n",
      "epoch: 53, loss: 1.499\n",
      "epoch: 54, loss: 1.421\n",
      "epoch: 55, loss: 1.572\n",
      "epoch: 56, loss: 1.425\n",
      "epoch: 57, loss: 1.529\n",
      "epoch: 58, loss: 1.271\n",
      "epoch: 59, loss: 1.290\n",
      "epoch: 60, loss: 1.321\n",
      "epoch: 61, loss: 1.390\n",
      "epoch: 62, loss: 1.235\n",
      "epoch: 63, loss: 1.295\n",
      "epoch: 64, loss: 1.218\n",
      "epoch: 65, loss: 1.292\n",
      "epoch: 66, loss: 1.100\n",
      "epoch: 67, loss: 1.251\n",
      "epoch: 68, loss: 1.245\n",
      "epoch: 69, loss: 1.184\n",
      "epoch: 70, loss: 1.016\n",
      "epoch: 71, loss: 1.138\n",
      "epoch: 72, loss: 0.979\n",
      "epoch: 73, loss: 1.049\n",
      "epoch: 74, loss: 0.974\n",
      "epoch: 75, loss: 1.121\n",
      "epoch: 76, loss: 1.047\n",
      "epoch: 77, loss: 0.968\n",
      "epoch: 78, loss: 0.973\n",
      "epoch: 79, loss: 0.900\n",
      "epoch: 80, loss: 0.925\n",
      "epoch: 81, loss: 0.951\n",
      "epoch: 82, loss: 0.889\n",
      "epoch: 83, loss: 0.952\n",
      "epoch: 84, loss: 0.926\n",
      "epoch: 85, loss: 0.899\n",
      "epoch: 86, loss: 0.847\n",
      "epoch: 87, loss: 0.807\n",
      "epoch: 88, loss: 0.774\n",
      "epoch: 89, loss: 0.823\n",
      "epoch: 90, loss: 0.701\n",
      "epoch: 91, loss: 0.703\n",
      "epoch: 92, loss: 0.715\n",
      "epoch: 93, loss: 0.710\n",
      "epoch: 94, loss: 0.736\n",
      "epoch: 95, loss: 0.590\n",
      "epoch: 96, loss: 0.775\n",
      "epoch: 97, loss: 0.711\n",
      "epoch: 98, loss: 0.646\n",
      "epoch: 99, loss: 0.630\n",
      "epoch: 100, loss: 0.654\n",
      "epoch: 101, loss: 0.578\n",
      "epoch: 102, loss: 0.604\n",
      "epoch: 103, loss: 0.672\n",
      "epoch: 104, loss: 0.593\n",
      "epoch: 105, loss: 0.504\n",
      "epoch: 106, loss: 0.533\n",
      "epoch: 107, loss: 0.515\n",
      "epoch: 108, loss: 0.522\n",
      "epoch: 109, loss: 0.554\n",
      "epoch: 110, loss: 0.473\n",
      "epoch: 111, loss: 0.444\n",
      "epoch: 112, loss: 0.525\n",
      "epoch: 113, loss: 0.502\n",
      "epoch: 114, loss: 0.464\n",
      "epoch: 115, loss: 0.454\n",
      "epoch: 116, loss: 0.433\n",
      "epoch: 117, loss: 0.426\n",
      "epoch: 118, loss: 0.530\n",
      "epoch: 119, loss: 0.444\n",
      "epoch: 120, loss: 0.488\n",
      "epoch: 121, loss: 0.375\n",
      "epoch: 122, loss: 0.369\n",
      "epoch: 123, loss: 0.494\n",
      "epoch: 124, loss: 0.430\n",
      "epoch: 125, loss: 0.420\n",
      "epoch: 126, loss: 0.453\n",
      "epoch: 127, loss: 0.408\n",
      "epoch: 128, loss: 0.407\n",
      "epoch: 129, loss: 0.379\n",
      "epoch: 130, loss: 0.391\n",
      "epoch: 131, loss: 0.302\n",
      "epoch: 132, loss: 0.394\n",
      "epoch: 133, loss: 0.406\n",
      "epoch: 134, loss: 0.358\n",
      "epoch: 135, loss: 0.320\n",
      "epoch: 136, loss: 0.300\n",
      "epoch: 137, loss: 0.307\n",
      "epoch: 138, loss: 0.317\n",
      "epoch: 139, loss: 0.289\n",
      "epoch: 140, loss: 0.295\n",
      "epoch: 141, loss: 0.258\n",
      "epoch: 142, loss: 0.312\n",
      "epoch: 143, loss: 0.268\n",
      "epoch: 144, loss: 0.253\n",
      "epoch: 145, loss: 0.248\n",
      "epoch: 146, loss: 0.247\n",
      "epoch: 147, loss: 0.285\n",
      "epoch: 148, loss: 0.266\n",
      "epoch: 149, loss: 0.293\n",
      "epoch: 150, loss: 0.273\n",
      "epoch: 151, loss: 0.272\n",
      "epoch: 152, loss: 0.260\n",
      "epoch: 153, loss: 0.219\n",
      "epoch: 154, loss: 0.167\n",
      "epoch: 155, loss: 0.255\n",
      "epoch: 156, loss: 0.201\n",
      "epoch: 157, loss: 0.188\n",
      "epoch: 158, loss: 0.182\n",
      "epoch: 159, loss: 0.234\n",
      "epoch: 160, loss: 0.232\n",
      "epoch: 161, loss: 0.206\n",
      "epoch: 162, loss: 0.246\n",
      "epoch: 163, loss: 0.211\n",
      "epoch: 164, loss: 0.184\n",
      "epoch: 165, loss: 0.243\n",
      "epoch: 166, loss: 0.187\n",
      "epoch: 167, loss: 0.200\n",
      "epoch: 168, loss: 0.162\n",
      "epoch: 169, loss: 0.163\n",
      "epoch: 170, loss: 0.217\n",
      "epoch: 171, loss: 0.208\n",
      "epoch: 172, loss: 0.153\n",
      "epoch: 173, loss: 0.147\n",
      "epoch: 174, loss: 0.225\n",
      "epoch: 175, loss: 0.139\n",
      "epoch: 176, loss: 0.184\n",
      "epoch: 177, loss: 0.149\n",
      "epoch: 178, loss: 0.173\n",
      "epoch: 179, loss: 0.154\n",
      "epoch: 180, loss: 0.141\n",
      "epoch: 181, loss: 0.142\n",
      "epoch: 182, loss: 0.169\n",
      "epoch: 183, loss: 0.158\n",
      "epoch: 184, loss: 0.176\n",
      "epoch: 185, loss: 0.140\n",
      "epoch: 186, loss: 0.179\n",
      "epoch: 187, loss: 0.141\n",
      "epoch: 188, loss: 0.143\n",
      "epoch: 189, loss: 0.172\n",
      "epoch: 190, loss: 0.167\n",
      "epoch: 191, loss: 0.143\n",
      "epoch: 192, loss: 0.118\n",
      "epoch: 193, loss: 0.119\n",
      "epoch: 194, loss: 0.143\n",
      "epoch: 195, loss: 0.122\n",
      "epoch: 196, loss: 0.105\n",
      "epoch: 197, loss: 0.112\n",
      "epoch: 198, loss: 0.166\n",
      "epoch: 199, loss: 0.129\n",
      "epoch: 200, loss: 0.113\n",
      "epoch: 201, loss: 0.112\n",
      "epoch: 202, loss: 0.079\n",
      "epoch: 203, loss: 0.153\n",
      "epoch: 204, loss: 0.107\n",
      "epoch: 205, loss: 0.137\n",
      "epoch: 206, loss: 0.094\n",
      "epoch: 207, loss: 0.118\n",
      "epoch: 208, loss: 0.103\n",
      "epoch: 209, loss: 0.121\n",
      "epoch: 210, loss: 0.116\n",
      "epoch: 211, loss: 0.105\n",
      "epoch: 212, loss: 0.099\n",
      "epoch: 213, loss: 0.095\n",
      "epoch: 214, loss: 0.114\n",
      "epoch: 215, loss: 0.100\n",
      "epoch: 216, loss: 0.085\n",
      "epoch: 217, loss: 0.072\n",
      "epoch: 218, loss: 0.078\n",
      "epoch: 219, loss: 0.121\n",
      "epoch: 220, loss: 0.145\n",
      "epoch: 221, loss: 0.092\n",
      "epoch: 222, loss: 0.089\n",
      "epoch: 223, loss: 0.115\n",
      "epoch: 224, loss: 0.068\n",
      "epoch: 225, loss: 0.110\n",
      "epoch: 226, loss: 0.103\n",
      "epoch: 227, loss: 0.135\n",
      "epoch: 228, loss: 0.060\n",
      "epoch: 229, loss: 0.075\n",
      "epoch: 230, loss: 0.085\n",
      "epoch: 231, loss: 0.095\n",
      "epoch: 232, loss: 0.102\n",
      "epoch: 233, loss: 0.078\n",
      "epoch: 234, loss: 0.103\n",
      "epoch: 235, loss: 0.090\n",
      "epoch: 236, loss: 0.091\n",
      "epoch: 237, loss: 0.068\n",
      "epoch: 238, loss: 0.086\n",
      "epoch: 239, loss: 0.081\n",
      "epoch: 240, loss: 0.072\n",
      "epoch: 241, loss: 0.079\n",
      "epoch: 242, loss: 0.077\n",
      "epoch: 243, loss: 0.074\n",
      "epoch: 244, loss: 0.086\n",
      "epoch: 245, loss: 0.098\n",
      "epoch: 246, loss: 0.068\n",
      "epoch: 247, loss: 0.088\n",
      "epoch: 248, loss: 0.102\n",
      "epoch: 249, loss: 0.071\n",
      "epoch: 250, loss: 0.087\n",
      "epoch: 251, loss: 0.087\n",
      "epoch: 252, loss: 0.074\n",
      "epoch: 253, loss: 0.097\n",
      "epoch: 254, loss: 0.074\n",
      "epoch: 255, loss: 0.076\n",
      "epoch: 256, loss: 0.067\n",
      "epoch: 257, loss: 0.062\n",
      "epoch: 258, loss: 0.070\n",
      "epoch: 259, loss: 0.054\n",
      "epoch: 260, loss: 0.051\n",
      "epoch: 261, loss: 0.063\n",
      "epoch: 262, loss: 0.085\n",
      "epoch: 263, loss: 0.047\n",
      "epoch: 264, loss: 0.061\n",
      "epoch: 265, loss: 0.083\n",
      "epoch: 266, loss: 0.062\n",
      "epoch: 267, loss: 0.048\n",
      "epoch: 268, loss: 0.063\n",
      "epoch: 269, loss: 0.058\n",
      "epoch: 270, loss: 0.061\n",
      "epoch: 271, loss: 0.041\n",
      "epoch: 272, loss: 0.044\n",
      "epoch: 273, loss: 0.073\n",
      "epoch: 274, loss: 0.050\n",
      "epoch: 275, loss: 0.080\n",
      "epoch: 276, loss: 0.051\n",
      "epoch: 277, loss: 0.061\n",
      "epoch: 278, loss: 0.059\n",
      "epoch: 279, loss: 0.061\n",
      "epoch: 280, loss: 0.072\n",
      "epoch: 281, loss: 0.076\n",
      "epoch: 282, loss: 0.059\n",
      "epoch: 283, loss: 0.040\n",
      "epoch: 284, loss: 0.049\n",
      "epoch: 285, loss: 0.046\n",
      "epoch: 286, loss: 0.048\n",
      "epoch: 287, loss: 0.059\n",
      "epoch: 288, loss: 0.065\n",
      "epoch: 289, loss: 0.055\n",
      "epoch: 290, loss: 0.054\n",
      "epoch: 291, loss: 0.075\n",
      "epoch: 292, loss: 0.059\n",
      "epoch: 293, loss: 0.041\n",
      "epoch: 294, loss: 0.063\n",
      "epoch: 295, loss: 0.053\n",
      "epoch: 296, loss: 0.045\n",
      "epoch: 297, loss: 0.057\n",
      "epoch: 298, loss: 0.071\n",
      "epoch: 299, loss: 0.065\n",
      "epoch: 300, loss: 0.053\n",
      "epoch: 301, loss: 0.052\n",
      "epoch: 302, loss: 0.037\n",
      "epoch: 303, loss: 0.059\n",
      "epoch: 304, loss: 0.048\n",
      "epoch: 305, loss: 0.062\n",
      "epoch: 306, loss: 0.064\n",
      "epoch: 307, loss: 0.074\n",
      "epoch: 308, loss: 0.052\n",
      "epoch: 309, loss: 0.043\n",
      "epoch: 310, loss: 0.049\n",
      "epoch: 311, loss: 0.038\n",
      "epoch: 312, loss: 0.036\n",
      "epoch: 313, loss: 0.054\n",
      "epoch: 314, loss: 0.060\n",
      "epoch: 315, loss: 0.029\n",
      "epoch: 316, loss: 0.041\n",
      "epoch: 317, loss: 0.038\n",
      "epoch: 318, loss: 0.046\n",
      "epoch: 319, loss: 0.059\n",
      "epoch: 320, loss: 0.038\n",
      "epoch: 321, loss: 0.030\n",
      "epoch: 322, loss: 0.038\n",
      "epoch: 323, loss: 0.036\n",
      "epoch: 324, loss: 0.039\n",
      "epoch: 325, loss: 0.043\n",
      "epoch: 326, loss: 0.032\n",
      "epoch: 327, loss: 0.039\n",
      "epoch: 328, loss: 0.036\n",
      "epoch: 329, loss: 0.055\n",
      "epoch: 330, loss: 0.042\n",
      "epoch: 331, loss: 0.044\n",
      "epoch: 332, loss: 0.051\n",
      "epoch: 333, loss: 0.025\n",
      "epoch: 334, loss: 0.033\n",
      "epoch: 335, loss: 0.042\n",
      "epoch: 336, loss: 0.033\n",
      "epoch: 337, loss: 0.022\n",
      "epoch: 338, loss: 0.031\n",
      "epoch: 339, loss: 0.046\n",
      "epoch: 340, loss: 0.033\n",
      "epoch: 341, loss: 0.039\n",
      "epoch: 342, loss: 0.034\n",
      "epoch: 343, loss: 0.032\n",
      "epoch: 344, loss: 0.043\n",
      "epoch: 345, loss: 0.034\n",
      "epoch: 346, loss: 0.041\n",
      "epoch: 347, loss: 0.035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 348, loss: 0.042\n",
      "epoch: 349, loss: 0.042\n",
      "epoch: 350, loss: 0.051\n",
      "epoch: 351, loss: 0.037\n",
      "epoch: 352, loss: 0.036\n",
      "epoch: 353, loss: 0.041\n",
      "epoch: 354, loss: 0.026\n",
      "epoch: 355, loss: 0.031\n",
      "epoch: 356, loss: 0.033\n",
      "epoch: 357, loss: 0.030\n",
      "epoch: 358, loss: 0.038\n",
      "epoch: 359, loss: 0.032\n",
      "epoch: 360, loss: 0.034\n",
      "epoch: 361, loss: 0.031\n",
      "epoch: 362, loss: 0.035\n",
      "epoch: 363, loss: 0.041\n",
      "epoch: 364, loss: 0.040\n",
      "epoch: 365, loss: 0.041\n",
      "epoch: 366, loss: 0.028\n",
      "epoch: 367, loss: 0.044\n",
      "epoch: 368, loss: 0.042\n",
      "epoch: 369, loss: 0.039\n",
      "epoch: 370, loss: 0.027\n",
      "epoch: 371, loss: 0.020\n",
      "epoch: 372, loss: 0.030\n",
      "epoch: 373, loss: 0.047\n",
      "epoch: 374, loss: 0.025\n",
      "epoch: 375, loss: 0.029\n",
      "epoch: 376, loss: 0.032\n",
      "epoch: 377, loss: 0.028\n",
      "epoch: 378, loss: 0.024\n",
      "epoch: 379, loss: 0.040\n",
      "epoch: 380, loss: 0.032\n",
      "epoch: 381, loss: 0.019\n",
      "epoch: 382, loss: 0.032\n",
      "epoch: 383, loss: 0.023\n",
      "epoch: 384, loss: 0.027\n",
      "epoch: 385, loss: 0.032\n",
      "epoch: 386, loss: 0.027\n",
      "epoch: 387, loss: 0.049\n",
      "epoch: 388, loss: 0.031\n",
      "epoch: 389, loss: 0.022\n",
      "epoch: 390, loss: 0.027\n",
      "epoch: 391, loss: 0.028\n",
      "epoch: 392, loss: 0.019\n",
      "epoch: 393, loss: 0.032\n",
      "epoch: 394, loss: 0.031\n",
      "epoch: 395, loss: 0.030\n",
      "epoch: 396, loss: 0.031\n",
      "epoch: 397, loss: 0.021\n",
      "epoch: 398, loss: 0.022\n",
      "epoch: 399, loss: 0.031\n",
      "epoch: 400, loss: 0.020\n",
      "epoch: 401, loss: 0.021\n",
      "epoch: 402, loss: 0.030\n",
      "epoch: 403, loss: 0.030\n",
      "epoch: 404, loss: 0.034\n",
      "epoch: 405, loss: 0.024\n",
      "epoch: 406, loss: 0.032\n",
      "epoch: 407, loss: 0.019\n",
      "epoch: 408, loss: 0.028\n",
      "epoch: 409, loss: 0.026\n",
      "epoch: 410, loss: 0.030\n",
      "epoch: 411, loss: 0.028\n",
      "epoch: 412, loss: 0.020\n",
      "epoch: 413, loss: 0.026\n",
      "epoch: 414, loss: 0.030\n",
      "epoch: 415, loss: 0.031\n",
      "epoch: 416, loss: 0.032\n",
      "epoch: 417, loss: 0.050\n",
      "epoch: 418, loss: 0.017\n",
      "epoch: 419, loss: 0.032\n",
      "epoch: 420, loss: 0.034\n",
      "epoch: 421, loss: 0.026\n",
      "epoch: 422, loss: 0.025\n",
      "epoch: 423, loss: 0.029\n",
      "epoch: 424, loss: 0.026\n",
      "epoch: 425, loss: 0.023\n",
      "epoch: 426, loss: 0.025\n",
      "epoch: 427, loss: 0.022\n",
      "epoch: 428, loss: 0.025\n",
      "epoch: 429, loss: 0.032\n",
      "epoch: 430, loss: 0.022\n",
      "epoch: 431, loss: 0.028\n",
      "epoch: 432, loss: 0.030\n",
      "epoch: 433, loss: 0.033\n",
      "epoch: 434, loss: 0.024\n",
      "epoch: 435, loss: 0.034\n",
      "epoch: 436, loss: 0.022\n",
      "epoch: 437, loss: 0.026\n",
      "epoch: 438, loss: 0.030\n",
      "epoch: 439, loss: 0.019\n",
      "epoch: 440, loss: 0.020\n",
      "epoch: 441, loss: 0.023\n",
      "epoch: 442, loss: 0.016\n",
      "epoch: 443, loss: 0.028\n",
      "epoch: 444, loss: 0.025\n",
      "epoch: 445, loss: 0.023\n",
      "epoch: 446, loss: 0.024\n",
      "epoch: 447, loss: 0.032\n",
      "epoch: 448, loss: 0.031\n",
      "epoch: 449, loss: 0.015\n",
      "epoch: 450, loss: 0.039\n",
      "epoch: 451, loss: 0.019\n",
      "epoch: 452, loss: 0.020\n",
      "epoch: 453, loss: 0.019\n",
      "epoch: 454, loss: 0.028\n",
      "epoch: 455, loss: 0.025\n",
      "epoch: 456, loss: 0.023\n",
      "epoch: 457, loss: 0.021\n",
      "epoch: 458, loss: 0.017\n",
      "epoch: 459, loss: 0.024\n",
      "epoch: 460, loss: 0.021\n",
      "epoch: 461, loss: 0.029\n",
      "epoch: 462, loss: 0.035\n",
      "epoch: 463, loss: 0.030\n",
      "epoch: 464, loss: 0.024\n",
      "epoch: 465, loss: 0.018\n",
      "epoch: 466, loss: 0.014\n",
      "epoch: 467, loss: 0.028\n",
      "epoch: 468, loss: 0.020\n",
      "epoch: 469, loss: 0.019\n",
      "epoch: 470, loss: 0.028\n",
      "epoch: 471, loss: 0.023\n",
      "epoch: 472, loss: 0.029\n",
      "epoch: 473, loss: 0.033\n",
      "epoch: 474, loss: 0.021\n",
      "epoch: 475, loss: 0.016\n",
      "epoch: 476, loss: 0.017\n",
      "epoch: 477, loss: 0.014\n",
      "epoch: 478, loss: 0.018\n",
      "epoch: 479, loss: 0.020\n",
      "epoch: 480, loss: 0.014\n",
      "epoch: 481, loss: 0.020\n",
      "epoch: 482, loss: 0.028\n",
      "epoch: 483, loss: 0.023\n",
      "epoch: 484, loss: 0.020\n",
      "epoch: 485, loss: 0.032\n",
      "epoch: 486, loss: 0.013\n",
      "epoch: 487, loss: 0.014\n",
      "epoch: 488, loss: 0.019\n",
      "epoch: 489, loss: 0.025\n",
      "epoch: 490, loss: 0.015\n",
      "epoch: 491, loss: 0.020\n",
      "epoch: 492, loss: 0.016\n",
      "epoch: 493, loss: 0.019\n",
      "epoch: 494, loss: 0.019\n",
      "epoch: 495, loss: 0.021\n",
      "epoch: 496, loss: 0.018\n",
      "epoch: 497, loss: 0.025\n",
      "epoch: 498, loss: 0.018\n",
      "epoch: 499, loss: 0.015\n",
      "epoch: 500, loss: 0.026\n",
      "epoch: 501, loss: 0.047\n",
      "epoch: 502, loss: 0.022\n",
      "epoch: 503, loss: 0.023\n",
      "epoch: 504, loss: 0.019\n",
      "epoch: 505, loss: 0.023\n",
      "epoch: 506, loss: 0.020\n",
      "epoch: 507, loss: 0.022\n",
      "epoch: 508, loss: 0.027\n",
      "epoch: 509, loss: 0.020\n",
      "epoch: 510, loss: 0.014\n",
      "epoch: 511, loss: 0.033\n",
      "epoch: 512, loss: 0.025\n",
      "epoch: 513, loss: 0.018\n",
      "epoch: 514, loss: 0.011\n",
      "epoch: 515, loss: 0.016\n",
      "epoch: 516, loss: 0.014\n",
      "epoch: 517, loss: 0.022\n",
      "epoch: 518, loss: 0.026\n",
      "epoch: 519, loss: 0.019\n",
      "epoch: 520, loss: 0.019\n",
      "epoch: 521, loss: 0.016\n",
      "epoch: 522, loss: 0.014\n",
      "epoch: 523, loss: 0.014\n",
      "epoch: 524, loss: 0.014\n",
      "epoch: 525, loss: 0.010\n",
      "epoch: 526, loss: 0.012\n",
      "epoch: 527, loss: 0.016\n",
      "epoch: 528, loss: 0.019\n",
      "epoch: 529, loss: 0.022\n",
      "epoch: 530, loss: 0.018\n",
      "epoch: 531, loss: 0.026\n",
      "epoch: 532, loss: 0.024\n",
      "epoch: 533, loss: 0.020\n",
      "epoch: 534, loss: 0.016\n",
      "epoch: 535, loss: 0.011\n",
      "epoch: 536, loss: 0.015\n",
      "epoch: 537, loss: 0.021\n",
      "epoch: 538, loss: 0.013\n",
      "epoch: 539, loss: 0.028\n",
      "epoch: 540, loss: 0.017\n",
      "epoch: 541, loss: 0.017\n",
      "epoch: 542, loss: 0.018\n",
      "epoch: 543, loss: 0.023\n",
      "epoch: 544, loss: 0.013\n",
      "epoch: 545, loss: 0.018\n",
      "epoch: 546, loss: 0.027\n",
      "epoch: 547, loss: 0.016\n",
      "epoch: 548, loss: 0.024\n",
      "epoch: 549, loss: 0.014\n",
      "epoch: 550, loss: 0.009\n",
      "epoch: 551, loss: 0.021\n",
      "epoch: 552, loss: 0.019\n",
      "epoch: 553, loss: 0.016\n",
      "epoch: 554, loss: 0.009\n",
      "epoch: 555, loss: 0.012\n",
      "epoch: 556, loss: 0.014\n",
      "epoch: 557, loss: 0.014\n",
      "epoch: 558, loss: 0.017\n",
      "epoch: 559, loss: 0.010\n",
      "epoch: 560, loss: 0.012\n",
      "epoch: 561, loss: 0.018\n",
      "epoch: 562, loss: 0.022\n",
      "epoch: 563, loss: 0.024\n",
      "epoch: 564, loss: 0.022\n",
      "epoch: 565, loss: 0.011\n",
      "epoch: 566, loss: 0.015\n",
      "epoch: 567, loss: 0.009\n",
      "epoch: 568, loss: 0.011\n",
      "epoch: 569, loss: 0.019\n",
      "epoch: 570, loss: 0.021\n",
      "epoch: 571, loss: 0.025\n",
      "epoch: 572, loss: 0.018\n",
      "epoch: 573, loss: 0.013\n",
      "epoch: 574, loss: 0.011\n",
      "epoch: 575, loss: 0.013\n",
      "epoch: 576, loss: 0.017\n",
      "epoch: 577, loss: 0.013\n",
      "epoch: 578, loss: 0.024\n",
      "epoch: 579, loss: 0.012\n",
      "epoch: 580, loss: 0.017\n",
      "epoch: 581, loss: 0.015\n",
      "epoch: 582, loss: 0.012\n",
      "epoch: 583, loss: 0.015\n",
      "epoch: 584, loss: 0.014\n",
      "epoch: 585, loss: 0.011\n",
      "epoch: 586, loss: 0.018\n",
      "epoch: 587, loss: 0.011\n",
      "epoch: 588, loss: 0.024\n",
      "epoch: 589, loss: 0.011\n",
      "epoch: 590, loss: 0.012\n",
      "epoch: 591, loss: 0.011\n",
      "epoch: 592, loss: 0.015\n",
      "epoch: 593, loss: 0.017\n",
      "epoch: 594, loss: 0.014\n",
      "epoch: 595, loss: 0.010\n",
      "epoch: 596, loss: 0.019\n",
      "epoch: 597, loss: 0.017\n",
      "epoch: 598, loss: 0.021\n",
      "epoch: 599, loss: 0.018\n",
      "epoch: 600, loss: 0.016\n",
      "epoch: 601, loss: 0.009\n",
      "epoch: 602, loss: 0.023\n",
      "epoch: 603, loss: 0.009\n",
      "epoch: 604, loss: 0.012\n",
      "epoch: 605, loss: 0.015\n",
      "epoch: 606, loss: 0.013\n",
      "epoch: 607, loss: 0.025\n",
      "epoch: 608, loss: 0.010\n",
      "epoch: 609, loss: 0.017\n",
      "epoch: 610, loss: 0.018\n",
      "epoch: 611, loss: 0.013\n",
      "epoch: 612, loss: 0.016\n",
      "epoch: 613, loss: 0.011\n",
      "epoch: 614, loss: 0.010\n",
      "epoch: 615, loss: 0.015\n",
      "epoch: 616, loss: 0.009\n",
      "epoch: 617, loss: 0.014\n",
      "epoch: 618, loss: 0.010\n",
      "epoch: 619, loss: 0.012\n",
      "epoch: 620, loss: 0.009\n",
      "epoch: 621, loss: 0.011\n",
      "epoch: 622, loss: 0.014\n",
      "epoch: 623, loss: 0.037\n",
      "epoch: 624, loss: 0.013\n",
      "epoch: 625, loss: 0.017\n",
      "epoch: 626, loss: 0.007\n",
      "epoch: 627, loss: 0.008\n",
      "epoch: 628, loss: 0.013\n",
      "epoch: 629, loss: 0.012\n",
      "epoch: 630, loss: 0.011\n",
      "epoch: 631, loss: 0.017\n",
      "epoch: 632, loss: 0.021\n",
      "epoch: 633, loss: 0.012\n",
      "epoch: 634, loss: 0.011\n",
      "epoch: 635, loss: 0.007\n",
      "epoch: 636, loss: 0.015\n",
      "epoch: 637, loss: 0.008\n",
      "epoch: 638, loss: 0.013\n",
      "epoch: 639, loss: 0.018\n",
      "epoch: 640, loss: 0.011\n",
      "epoch: 641, loss: 0.012\n",
      "epoch: 642, loss: 0.009\n",
      "epoch: 643, loss: 0.015\n",
      "epoch: 644, loss: 0.013\n",
      "epoch: 645, loss: 0.011\n",
      "epoch: 646, loss: 0.012\n",
      "epoch: 647, loss: 0.017\n",
      "epoch: 648, loss: 0.011\n",
      "epoch: 649, loss: 0.010\n",
      "epoch: 650, loss: 0.010\n",
      "epoch: 651, loss: 0.014\n",
      "epoch: 652, loss: 0.014\n",
      "epoch: 653, loss: 0.013\n",
      "epoch: 654, loss: 0.013\n",
      "epoch: 655, loss: 0.011\n",
      "epoch: 656, loss: 0.011\n",
      "epoch: 657, loss: 0.010\n",
      "epoch: 658, loss: 0.018\n",
      "epoch: 659, loss: 0.011\n",
      "epoch: 660, loss: 0.008\n",
      "epoch: 661, loss: 0.009\n",
      "epoch: 662, loss: 0.014\n",
      "epoch: 663, loss: 0.014\n",
      "epoch: 664, loss: 0.015\n",
      "epoch: 665, loss: 0.018\n",
      "epoch: 666, loss: 0.013\n",
      "epoch: 667, loss: 0.015\n",
      "epoch: 668, loss: 0.010\n",
      "epoch: 669, loss: 0.019\n",
      "epoch: 670, loss: 0.013\n",
      "epoch: 671, loss: 0.011\n",
      "epoch: 672, loss: 0.011\n",
      "epoch: 673, loss: 0.007\n",
      "epoch: 674, loss: 0.009\n",
      "epoch: 675, loss: 0.010\n",
      "epoch: 676, loss: 0.011\n",
      "epoch: 677, loss: 0.015\n",
      "epoch: 678, loss: 0.020\n",
      "epoch: 679, loss: 0.009\n",
      "epoch: 680, loss: 0.011\n",
      "epoch: 681, loss: 0.010\n",
      "epoch: 682, loss: 0.010\n",
      "epoch: 683, loss: 0.008\n",
      "epoch: 684, loss: 0.014\n",
      "epoch: 685, loss: 0.018\n",
      "epoch: 686, loss: 0.014\n",
      "epoch: 687, loss: 0.007\n",
      "epoch: 688, loss: 0.013\n",
      "epoch: 689, loss: 0.015\n",
      "epoch: 690, loss: 0.019\n",
      "epoch: 691, loss: 0.014\n",
      "epoch: 692, loss: 0.011\n",
      "epoch: 693, loss: 0.011\n",
      "epoch: 694, loss: 0.007\n",
      "epoch: 695, loss: 0.009\n",
      "epoch: 696, loss: 0.008\n",
      "epoch: 697, loss: 0.010\n",
      "epoch: 698, loss: 0.011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 699, loss: 0.008\n",
      "epoch: 700, loss: 0.008\n",
      "epoch: 701, loss: 0.012\n",
      "epoch: 702, loss: 0.007\n",
      "epoch: 703, loss: 0.009\n",
      "epoch: 704, loss: 0.009\n",
      "epoch: 705, loss: 0.007\n",
      "epoch: 706, loss: 0.010\n",
      "epoch: 707, loss: 0.008\n",
      "epoch: 708, loss: 0.009\n",
      "epoch: 709, loss: 0.010\n",
      "epoch: 710, loss: 0.008\n",
      "epoch: 711, loss: 0.008\n",
      "epoch: 712, loss: 0.010\n",
      "epoch: 713, loss: 0.016\n",
      "epoch: 714, loss: 0.011\n",
      "epoch: 715, loss: 0.006\n",
      "epoch: 716, loss: 0.009\n",
      "epoch: 717, loss: 0.013\n",
      "epoch: 718, loss: 0.008\n",
      "epoch: 719, loss: 0.012\n",
      "epoch: 720, loss: 0.008\n",
      "epoch: 721, loss: 0.009\n",
      "epoch: 722, loss: 0.009\n",
      "epoch: 723, loss: 0.007\n",
      "epoch: 724, loss: 0.008\n",
      "epoch: 725, loss: 0.008\n",
      "epoch: 726, loss: 0.006\n",
      "epoch: 727, loss: 0.006\n",
      "epoch: 728, loss: 0.009\n",
      "epoch: 729, loss: 0.009\n",
      "epoch: 730, loss: 0.012\n",
      "epoch: 731, loss: 0.006\n",
      "epoch: 732, loss: 0.012\n",
      "epoch: 733, loss: 0.013\n",
      "epoch: 734, loss: 0.014\n",
      "epoch: 735, loss: 0.010\n",
      "epoch: 736, loss: 0.009\n",
      "epoch: 737, loss: 0.011\n",
      "epoch: 738, loss: 0.006\n",
      "epoch: 739, loss: 0.008\n",
      "epoch: 740, loss: 0.006\n",
      "epoch: 741, loss: 0.009\n",
      "epoch: 742, loss: 0.006\n",
      "epoch: 743, loss: 0.006\n",
      "epoch: 744, loss: 0.008\n",
      "epoch: 745, loss: 0.014\n",
      "epoch: 746, loss: 0.008\n",
      "epoch: 747, loss: 0.008\n",
      "epoch: 748, loss: 0.010\n",
      "epoch: 749, loss: 0.009\n",
      "epoch: 750, loss: 0.014\n",
      "epoch: 751, loss: 0.007\n",
      "epoch: 752, loss: 0.007\n",
      "epoch: 753, loss: 0.010\n",
      "epoch: 754, loss: 0.009\n",
      "epoch: 755, loss: 0.008\n",
      "epoch: 756, loss: 0.008\n",
      "epoch: 757, loss: 0.007\n",
      "epoch: 758, loss: 0.010\n",
      "epoch: 759, loss: 0.009\n",
      "epoch: 760, loss: 0.015\n",
      "epoch: 761, loss: 0.010\n",
      "epoch: 762, loss: 0.008\n",
      "epoch: 763, loss: 0.010\n",
      "epoch: 764, loss: 0.010\n",
      "epoch: 765, loss: 0.010\n",
      "epoch: 766, loss: 0.011\n",
      "epoch: 767, loss: 0.009\n",
      "epoch: 768, loss: 0.009\n",
      "epoch: 769, loss: 0.008\n",
      "epoch: 770, loss: 0.008\n",
      "epoch: 771, loss: 0.013\n",
      "epoch: 772, loss: 0.012\n",
      "epoch: 773, loss: 0.006\n",
      "epoch: 774, loss: 0.007\n",
      "epoch: 775, loss: 0.007\n",
      "epoch: 776, loss: 0.008\n",
      "epoch: 777, loss: 0.008\n",
      "epoch: 778, loss: 0.006\n",
      "epoch: 779, loss: 0.007\n",
      "epoch: 780, loss: 0.010\n",
      "epoch: 781, loss: 0.012\n",
      "epoch: 782, loss: 0.012\n",
      "epoch: 783, loss: 0.007\n",
      "epoch: 784, loss: 0.007\n",
      "epoch: 785, loss: 0.008\n",
      "epoch: 786, loss: 0.008\n",
      "epoch: 787, loss: 0.009\n",
      "epoch: 788, loss: 0.010\n",
      "epoch: 789, loss: 0.009\n",
      "epoch: 790, loss: 0.009\n",
      "epoch: 791, loss: 0.008\n",
      "epoch: 792, loss: 0.009\n",
      "epoch: 793, loss: 0.009\n",
      "epoch: 794, loss: 0.005\n",
      "epoch: 795, loss: 0.007\n",
      "epoch: 796, loss: 0.007\n",
      "epoch: 797, loss: 0.007\n",
      "epoch: 798, loss: 0.006\n",
      "epoch: 799, loss: 0.007\n",
      "epoch: 800, loss: 0.010\n",
      "epoch: 801, loss: 0.008\n",
      "epoch: 802, loss: 0.008\n",
      "epoch: 803, loss: 0.009\n",
      "epoch: 804, loss: 0.009\n",
      "epoch: 805, loss: 0.017\n",
      "epoch: 806, loss: 0.007\n",
      "epoch: 807, loss: 0.007\n",
      "epoch: 808, loss: 0.006\n",
      "epoch: 809, loss: 0.007\n",
      "epoch: 810, loss: 0.006\n",
      "epoch: 811, loss: 0.006\n",
      "epoch: 812, loss: 0.012\n",
      "epoch: 813, loss: 0.005\n",
      "epoch: 814, loss: 0.008\n",
      "epoch: 815, loss: 0.011\n",
      "epoch: 816, loss: 0.008\n",
      "epoch: 817, loss: 0.008\n",
      "epoch: 818, loss: 0.008\n",
      "epoch: 819, loss: 0.016\n",
      "epoch: 820, loss: 0.007\n",
      "epoch: 821, loss: 0.008\n",
      "epoch: 822, loss: 0.010\n",
      "epoch: 823, loss: 0.007\n",
      "epoch: 824, loss: 0.007\n",
      "epoch: 825, loss: 0.007\n",
      "epoch: 826, loss: 0.010\n",
      "epoch: 827, loss: 0.006\n",
      "epoch: 828, loss: 0.012\n",
      "epoch: 829, loss: 0.008\n",
      "epoch: 830, loss: 0.007\n",
      "epoch: 831, loss: 0.006\n",
      "epoch: 832, loss: 0.005\n",
      "epoch: 833, loss: 0.010\n",
      "epoch: 834, loss: 0.009\n",
      "epoch: 835, loss: 0.008\n",
      "epoch: 836, loss: 0.007\n",
      "epoch: 837, loss: 0.004\n",
      "epoch: 838, loss: 0.006\n",
      "epoch: 839, loss: 0.006\n",
      "epoch: 840, loss: 0.007\n",
      "epoch: 841, loss: 0.004\n",
      "epoch: 842, loss: 0.009\n",
      "epoch: 843, loss: 0.005\n",
      "epoch: 844, loss: 0.008\n",
      "epoch: 845, loss: 0.007\n",
      "epoch: 846, loss: 0.012\n",
      "epoch: 847, loss: 0.006\n",
      "epoch: 848, loss: 0.006\n",
      "epoch: 849, loss: 0.008\n",
      "epoch: 850, loss: 0.009\n",
      "epoch: 851, loss: 0.006\n",
      "epoch: 852, loss: 0.008\n",
      "epoch: 853, loss: 0.006\n",
      "epoch: 854, loss: 0.012\n",
      "epoch: 855, loss: 0.012\n",
      "epoch: 856, loss: 0.006\n",
      "epoch: 857, loss: 0.010\n",
      "epoch: 858, loss: 0.006\n",
      "epoch: 859, loss: 0.004\n",
      "epoch: 860, loss: 0.009\n",
      "epoch: 861, loss: 0.006\n",
      "epoch: 862, loss: 0.008\n",
      "epoch: 863, loss: 0.005\n",
      "epoch: 864, loss: 0.008\n",
      "epoch: 865, loss: 0.009\n",
      "epoch: 866, loss: 0.005\n",
      "epoch: 867, loss: 0.006\n",
      "epoch: 868, loss: 0.007\n",
      "epoch: 869, loss: 0.009\n",
      "epoch: 870, loss: 0.004\n",
      "epoch: 871, loss: 0.010\n",
      "epoch: 872, loss: 0.009\n",
      "epoch: 873, loss: 0.008\n",
      "epoch: 874, loss: 0.005\n",
      "epoch: 875, loss: 0.007\n",
      "epoch: 876, loss: 0.008\n",
      "epoch: 877, loss: 0.009\n",
      "epoch: 878, loss: 0.008\n",
      "epoch: 879, loss: 0.007\n",
      "epoch: 880, loss: 0.006\n",
      "epoch: 881, loss: 0.004\n",
      "epoch: 882, loss: 0.006\n",
      "epoch: 883, loss: 0.005\n",
      "epoch: 884, loss: 0.011\n",
      "epoch: 885, loss: 0.017\n",
      "epoch: 886, loss: 0.019\n",
      "epoch: 887, loss: 0.005\n",
      "epoch: 888, loss: 0.010\n",
      "epoch: 889, loss: 0.011\n",
      "epoch: 890, loss: 0.005\n",
      "epoch: 891, loss: 0.014\n",
      "epoch: 892, loss: 0.005\n",
      "epoch: 893, loss: 0.007\n",
      "epoch: 894, loss: 0.004\n",
      "epoch: 895, loss: 0.007\n",
      "epoch: 896, loss: 0.005\n",
      "epoch: 897, loss: 0.013\n",
      "epoch: 898, loss: 0.006\n",
      "epoch: 899, loss: 0.008\n",
      "epoch: 900, loss: 0.008\n",
      "epoch: 901, loss: 0.004\n",
      "epoch: 902, loss: 0.005\n",
      "epoch: 903, loss: 0.008\n",
      "epoch: 904, loss: 0.006\n",
      "epoch: 905, loss: 0.007\n",
      "epoch: 906, loss: 0.010\n",
      "epoch: 907, loss: 0.008\n",
      "epoch: 908, loss: 0.005\n",
      "epoch: 909, loss: 0.010\n",
      "epoch: 910, loss: 0.005\n",
      "epoch: 911, loss: 0.007\n",
      "epoch: 912, loss: 0.005\n",
      "epoch: 913, loss: 0.007\n",
      "epoch: 914, loss: 0.007\n",
      "epoch: 915, loss: 0.008\n",
      "epoch: 916, loss: 0.008\n",
      "epoch: 917, loss: 0.005\n",
      "epoch: 918, loss: 0.007\n",
      "epoch: 919, loss: 0.005\n",
      "epoch: 920, loss: 0.006\n",
      "epoch: 921, loss: 0.012\n",
      "epoch: 922, loss: 0.008\n",
      "epoch: 923, loss: 0.006\n",
      "epoch: 924, loss: 0.006\n",
      "epoch: 925, loss: 0.008\n",
      "epoch: 926, loss: 0.007\n",
      "epoch: 927, loss: 0.008\n",
      "epoch: 928, loss: 0.008\n",
      "epoch: 929, loss: 0.009\n",
      "epoch: 930, loss: 0.005\n",
      "epoch: 931, loss: 0.007\n",
      "epoch: 932, loss: 0.005\n",
      "epoch: 933, loss: 0.008\n",
      "epoch: 934, loss: 0.007\n",
      "epoch: 935, loss: 0.004\n",
      "epoch: 936, loss: 0.010\n",
      "epoch: 937, loss: 0.008\n",
      "epoch: 938, loss: 0.005\n",
      "epoch: 939, loss: 0.005\n",
      "epoch: 940, loss: 0.004\n",
      "epoch: 941, loss: 0.013\n",
      "epoch: 942, loss: 0.005\n",
      "epoch: 943, loss: 0.006\n",
      "epoch: 944, loss: 0.008\n",
      "epoch: 945, loss: 0.006\n",
      "epoch: 946, loss: 0.006\n",
      "epoch: 947, loss: 0.006\n",
      "epoch: 948, loss: 0.007\n",
      "epoch: 949, loss: 0.005\n",
      "epoch: 950, loss: 0.004\n",
      "epoch: 951, loss: 0.004\n",
      "epoch: 952, loss: 0.007\n",
      "epoch: 953, loss: 0.008\n",
      "epoch: 954, loss: 0.007\n",
      "epoch: 955, loss: 0.005\n",
      "epoch: 956, loss: 0.008\n",
      "epoch: 957, loss: 0.005\n",
      "epoch: 958, loss: 0.007\n",
      "epoch: 959, loss: 0.006\n",
      "epoch: 960, loss: 0.004\n",
      "epoch: 961, loss: 0.007\n",
      "epoch: 962, loss: 0.005\n",
      "epoch: 963, loss: 0.007\n",
      "epoch: 964, loss: 0.004\n",
      "epoch: 965, loss: 0.009\n",
      "epoch: 966, loss: 0.008\n",
      "epoch: 967, loss: 0.009\n",
      "epoch: 968, loss: 0.004\n",
      "epoch: 969, loss: 0.005\n",
      "epoch: 970, loss: 0.006\n",
      "epoch: 971, loss: 0.003\n",
      "epoch: 972, loss: 0.008\n",
      "epoch: 973, loss: 0.008\n",
      "epoch: 974, loss: 0.006\n",
      "epoch: 975, loss: 0.004\n",
      "epoch: 976, loss: 0.004\n",
      "epoch: 977, loss: 0.006\n",
      "epoch: 978, loss: 0.005\n",
      "epoch: 979, loss: 0.004\n",
      "epoch: 980, loss: 0.004\n",
      "epoch: 981, loss: 0.007\n",
      "epoch: 982, loss: 0.005\n",
      "epoch: 983, loss: 0.014\n",
      "epoch: 984, loss: 0.006\n",
      "epoch: 985, loss: 0.004\n",
      "epoch: 986, loss: 0.013\n",
      "epoch: 987, loss: 0.009\n",
      "epoch: 988, loss: 0.006\n",
      "epoch: 989, loss: 0.006\n",
      "epoch: 990, loss: 0.005\n",
      "epoch: 991, loss: 0.006\n",
      "epoch: 992, loss: 0.010\n",
      "epoch: 993, loss: 0.006\n",
      "epoch: 994, loss: 0.006\n",
      "epoch: 995, loss: 0.004\n",
      "epoch: 996, loss: 0.010\n",
      "epoch: 997, loss: 0.004\n",
      "epoch: 998, loss: 0.008\n",
      "epoch: 999, loss: 0.005\n",
      "Training avg loss: 0.212\n"
     ]
    }
   ],
   "source": [
    "def train(model, data, batch_size, n_epoch):\n",
    "    model.train() # Sets the module in training mode. This has any effect only on modules such as Dropout or BatchNorm.\n",
    "    if use_cuda:\n",
    "        model.cuda()\n",
    "    losses = []\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "    for epoch in range(n_epoch):\n",
    "        epoch_loss = 0.0\n",
    "        random.shuffle(data)\n",
    "        for i in range(0, len(data)-batch_size, batch_size): # discard some last elements\n",
    "            in_data, labels = [], []\n",
    "            for sentence, label in data[i: i+batch_size]:\n",
    "                index_vec = [w2i[w] if w in w2i.keys() else 0 for w in sentence[0:max_sentence_len]]\n",
    "                pad_len = max(0, max_sentence_len - len(index_vec))\n",
    "                index_vec += [0] * pad_len\n",
    "                index_vec = index_vec[:max_sentence_len] ## TBD for same len\n",
    "                in_data.append(index_vec)\n",
    "                labels.append(label)\n",
    "            sent_var = Variable(torch.LongTensor(in_data))\n",
    "            if use_cuda: sent_var = sent_var.cuda()\n",
    "\n",
    "            target_var = Variable(torch.LongTensor(labels))\n",
    "            if use_cuda: target_var = target_var.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            score = model(sent_var)\n",
    "            loss = criterion(score, target_var)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print('epoch: {:d}, loss: {:.3f}'.format(epoch, epoch_loss))\n",
    "        losses.append(epoch_loss)\n",
    "    print('Training avg loss: {:.3f}'.format(sum(losses)/len(losses)))\n",
    "        \n",
    "    return model, losses\n",
    "\n",
    "def test(model, data, n_test):\n",
    "    model.eval()\n",
    "    correct_1 = 0\n",
    "    correct_3 = 0\n",
    "    correct_5 = 0\n",
    "    for sentence, label in data[:n_test]:\n",
    "        index_vec = [w2i[w] if w in w2i.keys() else 0 for w in sentence[0:max_sentence_len]]\n",
    "        sent_var = Variable(torch.LongTensor([index_vec]))\n",
    "        if use_cuda: sent_var = sent_var.cuda()\n",
    "        score = model(sent_var)\n",
    "        pred = torch.topk(score,k=5,dim=1)[1].squeeze(0)\n",
    "        if label == pred[0].item():\n",
    "            correct_1 += 1\n",
    "        if label in pred[:3]:\n",
    "            correct_3 += 1\n",
    "        if label in pred[:5]:\n",
    "            correct_5 += 1\n",
    "        \n",
    "        \n",
    "    print('Test top 1 acc: {:.3f} ({:d}/{:d})'.format(correct_1/n_test, correct_1, n_test))\n",
    "    print('Test top 3 acc: {:.3f} ({:d}/{:d})'.format(correct_3/n_test, correct_3, n_test))\n",
    "    print('Test top 5 acc: {:.3f} ({:d}/{:d})'.format(correct_5/n_test, correct_5, n_test))\n",
    "out_ch = 100\n",
    "batch_size = 64\n",
    "n_epoch = 1000\n",
    "fil = [1,2,3,4]\n",
    "num_class = len(label_name)\n",
    "word2vec_path = '/home/shiyang/cnn-for-sentence-classification/dataset/glove.6B.50d.txt'\n",
    "model = Net(word2vec_path, out_ch, fil,num_class)\n",
    "i2w = {i:w for i,w in enumerate(model.my_dict)}\n",
    "w2i = {w:i for i,w in enumerate(model.my_dict)}\n",
    "model, losses = train(model, train_data, batch_size, n_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test top 1 acc: 0.307 (20157/65556)\n",
      "Test top 3 acc: 0.546 (35801/65556)\n",
      "Test top 5 acc: 0.672 (44070/65556)\n"
     ]
    }
   ],
   "source": [
    "test(model, test_data, len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
